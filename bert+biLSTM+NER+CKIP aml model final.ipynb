{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Bidirectional, LSTM, Dense\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import keras.callbacks\n",
    "import re\n",
    "import codecs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bert_dir = r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\bert'\n",
    "config_path = os.path.join(bert_dir, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(bert_dir, 'bert_model.ckpt')\n",
    "dict_path = os.path.join(bert_dir, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_0620.csv')\n",
    "data = data[data[\"status\"]==\"ok\"].drop([\"url\",\"context\",\"raw_content\",\"status\", \"content_status\"],axis = 1)\n",
    "data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data['name'] = data['name'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'C:\\Users\\rocker\\extra data\\人肉爬蟲_20200615.csv', encoding='cp950')\n",
    "data2['name'] = data2['name'].apply(lambda x: eval(x))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "test = data2\n",
    "test = test[['news_id', 'name', 'content', 'aml_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test =  train_test_split(data, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 建立 aml 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(dict_path):\n",
    "    \n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "            \n",
    "    return token_dict\n",
    "\n",
    "def transfer(i):\n",
    "    \n",
    "    if i != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def encoded(tokenizer, data, maxlen):\n",
    "    \n",
    "    x, y, z = [], [], []\n",
    "    if 'content' in data.columns:\n",
    "        for content in data['content']:\n",
    "            x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "            x3 = [transfer(i) for i in x1]\n",
    "            x.append(x1)\n",
    "            y.append(x2)\n",
    "            z.append(x3)\n",
    "    elif 'Sentence' in data.columns:\n",
    "        for content in data['Sentence']:\n",
    "            x1, x2 = tokenizer.encode(content, max_len=maxlen)\n",
    "            x3 = [transfer(i) for i in x1]\n",
    "            x.append(x1)\n",
    "            y.append(x2)\n",
    "            z.append(x3)\n",
    "            \n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = create_tokenizer(dict_path)\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 256\n",
    "batch_size = 8\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rocker\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\rocker\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def bert_LSTM_model():\n",
    "    \n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
    "    sequence_output = model.layers[-9].output\n",
    "    #sequence_output = Lambda(lambda x: x[:, 0])(sequence_output)\n",
    "    sequence_output = Bidirectional(LSTM(128, return_sequences=False))(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model = Model(model.input, output)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    return model\n",
    "\n",
    "model = bert_LSTM_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def bert_model():\n",
    "#    \n",
    "#    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
    "#    sequence_output = model.layers[-6].output\n",
    "#    sequence_output = Dense(128, activation='relu')(sequence_output)\n",
    "#    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "#    model = Model(model.input, output)\n",
    "#    return model\n",
    "#\n",
    "#model = bert_model()\n",
    "#\n",
    "#set_trainable = False\n",
    "#for layer in model.layers:\n",
    "#    if layer.name == 'Encoder-12-MultiHeadSelfAttention':\n",
    "#        set_trainable = True\n",
    "#    if set_trainable:\n",
    "#        layer.trainable = True\n",
    "#    else:\n",
    "#        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.asarray(train['aml_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id, segment_id, mask_input = encoded(tokenizer, train, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=train.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "                 keras.callbacks.EarlyStopping(monitor='val_acc', patience=1)\n",
    "                 #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          #validation_split=0.1,\n",
    "          #callbacks=callback_list\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 載入模型\n",
    "model.load_weights(r'C:\\Users\\rocker\\bert model\\model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 建立 NER 模型\n",
    "### transfer_NER 要跑 1.5 小時，訓練模型要跑 17mins/epoch，所以load weight就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把大於512的新聞以句點分段 (bert最多只能吃512)\n",
    "def split_content(data):\n",
    "    data_more_split = pd.DataFrame()\n",
    "    for i, row in data.iterrows():\n",
    "        if (len(row['content']) > 512) & (len(row['content']) <= 1024):\n",
    "\n",
    "            s = row['content']\n",
    "            s_split = [(i, abs(len(s)//2 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "            second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "            contents = [first, second]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content}, index=[66]), ignore_index=True)\n",
    "\n",
    "        elif len(row['content']) > 1024:\n",
    "\n",
    "            s = row['content']\n",
    "            s_split1 = [(i, abs(len(s)//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            s_split2 = [(i, abs(len(s)*2//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "            idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "            second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "            third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "            contents = [first, second, third]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content}, index=[66]), ignore_index=True)\n",
    "    \n",
    "    return data_more_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_ner = 512\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "input_shape = (maxlen_ner, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_BiLSTM_CRF_model():\n",
    "    \n",
    "    ner_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_ner)\n",
    "    bert_output = ner_model.layers[-9].output\n",
    "    X = Lambda(lambda x: x[:, 0: input_shape[0]])(bert_output)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    #X = TimeDistributed(Dense(len(y_token_dict), activation='relu'))(X)\n",
    "    output = CRF(3, sparse_target = True)(X)    \n",
    "    ner_model = Model(ner_model.input, output)\n",
    "    \n",
    "    for layer in ner_model.layers:\n",
    "        layer.trainable = False\n",
    "    ner_model.layers[-1].trainable = True\n",
    "    ner_model.layers[-2].trainable = True\n",
    "    \n",
    "    return ner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ner_model = bert_BiLSTM_CRF_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_NER 要跑 1.5 小時，訓練模型要跑 17mins/epoch，所以load weight就好\n",
    "ner_model.load_weights(r'C:\\Users\\rocker\\bert model\\ner_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 建立模型 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['aml_label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'E:\\Desktop\\檢察官_法官_filter3 - 複製.csv', encoding='cp950')\n",
    "data2['aml_label'] = data2['people_list'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data2 = data2[data2['aml_label']==1]\n",
    "data2 = data2.drop(['title', 'link'], axis=1)\n",
    "data2['people_list'] = data2['people_list'].apply(lambda x: eval(x))\n",
    "data2['news_id'] = range(20000, 20000+len(data2))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "data2.columns = ['news_id', 'content', 'aml_label', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv(r'E:\\Desktop\\檢察官_法官_filter.csv', encoding='cp950')\n",
    "data3 = data3.drop(['title', 'link'], axis=1)\n",
    "data3['aml_label'] = data3['people_list'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data3 = data3[data3['aml_label']==1]\n",
    "data3['people_list'] = data3['people_list'].apply(lambda x: eval(x))\n",
    "data3['news_id'] = range(30000, 30000+len(data3))\n",
    "data3.columns = ['news_id', 'content', 'aml_label', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv(r'E:\\Desktop\\nownews_all_content_part1_0708.csv', encoding='cp950')\n",
    "data4 = data4.drop(['title', 'link', 'people_list'], axis=1)\n",
    "data4['aml_label'] = data4['name'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data4 = data4[data4['aml_label']==1]\n",
    "data4['name'] = data4['name'].apply(lambda x: eval(x))\n",
    "data4['news_id'] = range(40000, 40000+len(data4))\n",
    "data4['content'] = data4['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data4['content'] = data4['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data4['content'] = data4['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.append(data2).append(data3).append(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_sentences = 256\n",
    "batch_size = 8\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把大於512的新聞以句點分段 (bert最多只能吃512)\n",
    "def split_content_model2(data):\n",
    "    data_more_split = pd.DataFrame()\n",
    "    for i, row in data.iterrows():\n",
    "        if (len(row['content']) > 512) & (len(row['content']) <= 1024):\n",
    "\n",
    "            s = row['content']\n",
    "            s_split = [(i, abs(len(s)//2 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "            second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "            contents = [first, second]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content, 'aml_label':row['aml_label']}, index=[66]), ignore_index=True)\n",
    "\n",
    "        elif len(row['content']) > 1024:\n",
    "\n",
    "            s = row['content']\n",
    "            s_split1 = [(i, abs(len(s)//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            s_split2 = [(i, abs(len(s)*2//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "            idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "            second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "            third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "            contents = [first, second, third]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content, 'aml_label':row['aml_label']}, index=[66]), ignore_index=True)\n",
    "    \n",
    "    return data_more_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2textByName(news, name):\n",
    "\n",
    "    def _addperiod(matched):\n",
    "        intStr = matched.group()\n",
    "        addedStr = str('。='+intStr)\n",
    "        return addedStr\n",
    "    \n",
    "    i = np.sum([[news.count(name) for news in news] for name in name], axis=0)\n",
    "    \n",
    "    if type(i) != np.ndarray:\n",
    "        return news\n",
    "    else:\n",
    "        for i,(num, text) in zip(list(i), enumerate(news)):\n",
    "            if (i > 1) & ('、' not in text) & (('之' in text) | ('的' in text)):\n",
    "                replacedStr = re.sub('|'.join(name), _addperiod, text)\n",
    "                replacedStr = re.sub('。=','', replacedStr, 1)\n",
    "                news[num] = replacedStr\n",
    "        newsAfter = sum([news.split('=') for news in news], [])\n",
    "\n",
    "        return newsAfter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences_model2(test, people_list, stick, tokenizer=tokenizer, maxlen=maxlen_sentences):\n",
    "    \n",
    "    AML = pd.DataFrame(columns=['news_id', 'Name', 'Sentence'])\n",
    "    aml_highrisk = np.asarray(test['content'])\n",
    "    news_ids = np.asarray(test['news_id'])\n",
    "    people_list = list(test['people_list'])\n",
    "\n",
    "    \n",
    "    for k, (news_id, y_news) in enumerate(zip(news_ids ,aml_highrisk)): \n",
    "        #用，。？切分句子\n",
    "        y_news = y_news.replace('。','=。')\n",
    "        y_news = y_news.replace('，','*，')\n",
    "        y_news = y_news.replace('？','+？')\n",
    "        y_news = y_news.replace('；','{；')\n",
    "        \n",
    "        news = re.split('，|。|？|；', y_news)\n",
    "        \n",
    "        news = [news.replace('=','。') for news in news]\n",
    "        news = [news.replace('*','，') for news in news]\n",
    "        news = [news.replace('+','？') for news in news]\n",
    "        news = [news.replace('{','；') for news in news]\n",
    "        news = split2textByName(news, people_list[k])\n",
    "#############################################################################################################\n",
    "        for i in range(len(people_list[k])):\n",
    "            # 找出人名存在的 news index\n",
    "            index = [index for index, _ in enumerate(news) if people_list[k][i] in _]                                \n",
    "            x = [people for people in people_list[k] if people_list[k][i] not in people]\n",
    "\n",
    "            name = [name for name in people_list[k] if name != people_list[k][i]]\n",
    "            name = [name for name in name if ((len(name)<3) & (name[0] != people_list[k][i][0])) | (len(name)>=3)]\n",
    "            name.sort(reverse=True)\n",
    "\n",
    "            #if name != []:\n",
    "            #    text = [re.sub('|'.join(name), '其他人名', news) for news in news]\n",
    "            #else:\n",
    "            #    text = news\n",
    "\n",
    "            for j in index:\n",
    "\n",
    "                mid = news[j]\n",
    "\n",
    "                if name != []:\n",
    "                    name = [name.replace('?', '\\?') for name in name]\n",
    "                    mid = re.sub('|'.join(name), '', mid)\n",
    "                \n",
    "                if j == 0:\n",
    "                    if len(news) != 1:\n",
    "                        end = news[j+1]  \n",
    "\n",
    "                        if True in [people in news[j+1] for people in x]:\n",
    "                            sentences = mid\n",
    "                        else:\n",
    "                            sentences = mid + end\n",
    "                    elif len(news) == 1:\n",
    "                        sentences = mid\n",
    "\n",
    "                elif j+1 == len(news):\n",
    "                    start = news[j-1]\n",
    "\n",
    "                    if True in [people in news[j-1] for people in x]:\n",
    "                        sentences = mid\n",
    "                    else:\n",
    "                        sentences = start + mid\n",
    "\n",
    "                else:\n",
    "                    end = news[j+1]\n",
    "                    start = news[j-1]                    \n",
    "\n",
    "                    if '。' in start:\n",
    "                        start = ''\n",
    "                    elif '。' in mid:\n",
    "                        end = ''                 \n",
    "\n",
    "                    if (True not in [people in news[j-1] for people in x]) & (True not in [people in news[j+1] for people in x]):\n",
    "                        sentences = start + mid + end\n",
    "                    elif (True not in [people in news[j-1] for people in x]) & (True in [people in news[j+1] for people in x]):\n",
    "                        sentences = start + mid\n",
    "                    elif (True in [people in news[j-1] for people in x]) & (True not in [people in news[j+1] for people in x]):\n",
    "                        sentences = mid + end\n",
    "                    else:\n",
    "                        sentences = mid    \n",
    "                        \n",
    "                AML = AML.append(pd.DataFrame([[news_ids[k] ,people_list[k][i], sentences]], columns=AML.columns))\n",
    "                #break  \n",
    "                \n",
    "    #若 stick==True 則把多筆同姓名句子以逗點合併 (效果不好)\n",
    "    if stick:\n",
    "        AML = AML.groupby(['news_id', 'Name'])['Sentence'].apply('，'.join).reset_index()\n",
    "    \n",
    "    \n",
    "    # 把指向同一人的姓名改成一樣（陳男 -> 陳水扁），若指向多人則不改（陳男 -> 陳致中、陳水扁）\n",
    "    # 將預測不完整的名字回填（王音 -> 王音之）\n",
    "    name_list = []\n",
    "    for ids in AML['news_id'].unique():\n",
    "        full_name = [name for name in AML[(AML['news_id'] == ids)]['Name']]\n",
    "        full_3name = [name for name in AML[(AML['news_id'] == ids)]['Name'] if len(name) == 3]\n",
    "        \n",
    "        a = Counter([name[0] for name in full_3name])\n",
    "        keep = [k for k,v in a.items() if v == 1]\n",
    "        full_3name_filter = [name for name in full_3name if name[0] in keep]\n",
    "        name_dict = dict((name[0], name) for name in full_3name_filter)   # ex: {'陳' : '陳水扁'}\n",
    "        name_dict_2 = dict(zip([name[0:2] for name in full_3name], full_3name))  # ex: {'王音': '王音之'}\n",
    "        name_dict_3 = dict(zip([name[1:] for name in full_3name], full_3name))\n",
    "        for name in full_name:\n",
    "            if (name[0] in name_dict.keys()) & (len(name) == 1):\n",
    "                name_list.append(name_dict.get(name[0]))\n",
    "            elif (name[0] in name_dict.keys()) & (len(name) == 2) & (name[-1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                                                                                  '員', '稱', '家', '哥', '媽', '生', '處',\\\n",
    "                                                                                  '和', '揆', '要', '再', '董', '涉', '母',\\\n",
    "                                                                                  '辱', '公', '少', '為', '指', '翁', '粉',\\\n",
    "                                                                                  '趁', '仔', '依', '氏', '父']):\n",
    "                name_list.append(name_dict.get(name[0]))\n",
    "            elif (name in name_dict_2.keys()) & (len(name) == 2):\n",
    "                name_list.append(name_dict_2.get(name))\n",
    "            elif (name in name_dict_3.keys()):\n",
    "                name_list.append(name_dict_3.get(name))\n",
    "            else:\n",
    "                name_list.append(name)\n",
    "                \n",
    "    \n",
    "    # 排除重複資料、排除一字、兩字簡稱、兩字三字四字姓不在姓氏表中的人\n",
    "    AML['Name'] = name_list\n",
    "    AML = AML.drop_duplicates()\n",
    "    AML = AML[AML['Name'].apply(lambda x: (len(x) > 1) )]\n",
    "    AML = AML[~AML['Name'].apply(lambda x: (len(x) == 2) & (x[1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                                                                     '員', '稱', '家', '哥', '媽', '生', '處',\\\n",
    "                                                                     '和', '揆', '要', '再', '董', '涉', '母',\\\n",
    "                                                                     '辱', '公', '少', '為', '指', '翁', '粉',\\\n",
    "                                                                     '趁', '仔', '依', '氏', '父']))]\n",
    "    \n",
    "    return AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得名字 (預測結果為onehot的狀態)\n",
    "def get_name(input_id, y_pred):\n",
    "    \n",
    "    label_list = []\n",
    "    word_dict = {v: k for k, v in token_dict.items()}\n",
    "    \n",
    "    for input_data, y in zip(input_id, y_pred):\n",
    "        people_index = ''.join([str(a) for a in list(y)])\n",
    "        j = 0\n",
    "        name_list = []\n",
    "        split_index = re.findall('[12]2*', people_index)\n",
    "        name = ''.join([word_dict.get(input_data[index]) for index, value in enumerate(y) if value != 0])\n",
    "        \n",
    "        # [UNK], [PAD]會被算成 5 個字元，避免轉換成文字的index因長度不同對不上，故用 1 個字元的其他符號替代\n",
    "        # 王春甡 -> 王春[UNK] -> 王春?\n",
    "        name = name.replace('[UNK]','?')\n",
    "        name = name.replace('[PAD]','!')\n",
    "        \n",
    "        for i in split_index:\n",
    "            name_list.append(name[0+j:len(i)+j])\n",
    "            j = len(i) + j\n",
    "            \n",
    "        name_list = [name for name in name_list]\n",
    "        label_list.append(list(set(name_list)))\n",
    "    \n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. 將超過 512 的句子以句點拆成多句分段預測\n",
    "train_ner = train.drop(['name'], axis=1)\n",
    "data_less = train_ner[train_ner['content'].str.len() <= 512]\n",
    "data_more = train_ner[train_ner['content'].str.len() > 512]\n",
    "data_more_split = split_content_model2(data_more)\n",
    "train_ner = data_less.append(data_more_split).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# 3. NER 預測人名\n",
    "input_id, segment_id, mask_input = encoded(tokenizer, train_ner, maxlen=maxlen_ner)\n",
    "prediction = ner_model.predict([input_id, segment_id, mask_input])\n",
    "y_pred = np.argmax(prediction, axis=-1)\n",
    "people_list = get_name(input_id, y_pred)\n",
    "\n",
    "\n",
    "# 4. 將拆開的句子組合回去\n",
    "train_ner['people_list'] = people_list\n",
    "content = train_ner[['news_id', 'content', 'aml_label']]\n",
    "content = content.groupby(['news_id', 'aml_label'])['content'].apply(lambda x : '。'.join(x)).reset_index()\n",
    "people = train_ner[['news_id', 'aml_label', 'people_list']]\n",
    "people = people.groupby(['news_id', 'aml_label'])['people_list'].agg(sum).reset_index()\n",
    "people['people_list'] = [list(set(people)) for people in people['people_list']]\n",
    "train_ner = pd.merge(content, people, on=['news_id', 'aml_label'], how='left')\n",
    "\n",
    "# 5. 將 [UNK], [PAD] 轉換回來 (王春? -> 王春甡)\n",
    "for _, row in train_ner.iterrows():\n",
    "    for i, name in enumerate(row['people_list']):\n",
    "        if ('?' in name) | ('!' in name):\n",
    "            reexp = name.replace('?', '.').replace('!', '.')\n",
    "            reexp = r\"{}\".format(reexp)\n",
    "            row['people_list'][i] = re.search(reexp, row['content']).group()\n",
    "            \n",
    "AML = predict_sentences_model2(train_ner, list(train_ner['people_list']), tokenizer=tokenizer, maxlen=maxlen_sentences, stick=False)\n",
    "\n",
    "#???\n",
    "AML = AML.groupby(['news_id', 'Name'])['Sentence'].apply('，'.join).reset_index()\n",
    "\n",
    "train_name = train.drop(['content'], axis=1)\n",
    "s = train_name.apply(lambda x: pd.Series(x['name']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'Name'\n",
    "train_name = train_name.drop('name', axis=1).join(s)\n",
    "\n",
    "aml_train = pd.merge(AML, train_name, on=['news_id', 'Name'], how='left')\n",
    "aml_train = aml_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra = aml_train[((aml_train.Sentence.str.contains('警')) | (aml_train.Sentence.str.contains('檢察官')))]\n",
    "#aml_train = aml_train.append(extra).append(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.asarray(aml_train['aml_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id, segment_id, mask_input = encoded(tokenizer, aml_train, maxlen_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def bert_LSTM_model2():\n",
    "#    \n",
    "#    model2 = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_sentences)\n",
    "#    sequence_output = model2.layers[-9].output\n",
    "#    #sequence_output = Lambda(lambda x: x[:, 0])(sequence_output)\n",
    "#    sequence_output = Bidirectional(LSTM(128, return_sequences=False))(sequence_output)\n",
    "#    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "#    model2 = Model(model2.input, output)\n",
    "#    \n",
    "#    for layer in model2.layers:\n",
    "#        layer.trainable = False\n",
    "#    model2.layers[-1].trainable = True\n",
    "#    model2.layers[-2].trainable = True\n",
    "#    \n",
    "#    return model2\n",
    "#\n",
    "#model2 = bert_LSTM_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model():\n",
    "    \n",
    "    model2 = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_sentences)\n",
    "    sequence_output = model2.layers[-6].output\n",
    "    sequence_output = Dense(64, activation='relu')(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model2 = Model(model2.input, output)\n",
    "    return model2\n",
    "\n",
    "model2 = bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "for layer in model2.layers:\n",
    "    if layer.name == 'Encoder-11-MultiHeadSelfAttention':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=train.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "                 keras.callbacks.EarlyStopping(monitor='val_acc', patience=1)\n",
    "                 #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model2.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          #validation_split=0.1,\n",
    "          #callbacks=callback_list\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(r'C:\\Users\\rocker\\bert model\\model2_61.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_aml(model, test, aml_threshold):\n",
    "    \n",
    "    #第一階段預測，大於aml_threshold者為疑似aml文章\n",
    "    input_id, segment_id, mask_input = encoded(tokenizer, test, maxlen=maxlen)\n",
    "    prediction = model.predict([input_id, segment_id, mask_input])\n",
    "    prediction[prediction >= aml_threshold] = 1\n",
    "    prediction[prediction < aml_threshold] = 0\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得名字 (預測結果為onehot的狀態)\n",
    "def get_name(input_id, y_pred):\n",
    "    \n",
    "    label_list = []\n",
    "    word_dict = {v: k for k, v in token_dict.items()}\n",
    "    \n",
    "    for input_data, y in zip(input_id, y_pred):\n",
    "        people_index = ''.join([str(a) for a in list(y)])\n",
    "        j = 0\n",
    "        name_list = []\n",
    "        split_index = re.findall('[12]2*', people_index)\n",
    "        name = ''.join([word_dict.get(input_data[index]) for index, value in enumerate(y) if value != 0])\n",
    "        \n",
    "        # [UNK], [PAD]會被算成 5 個字元，避免轉換成文字的index因長度不同對不上，故用 1 個字元的其他符號替代\n",
    "        # 王春甡 -> 王春[UNK] -> 王春?\n",
    "        name = name.replace('[UNK]','?')\n",
    "        name = name.replace('[PAD]','!')\n",
    "        \n",
    "        for i in split_index:\n",
    "            name_list.append(name[0+j:len(i)+j])\n",
    "            j = len(i) + j\n",
    "            \n",
    "        name_list = [name for name in name_list]\n",
    "        label_list.append(list(set(name_list)))\n",
    "    \n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(test, people_list, stick, threshold, tokenizer=tokenizer, maxlen=maxlen_sentences):\n",
    "    \n",
    "    time = datetime.now()\n",
    "    \n",
    "    AML = pd.DataFrame(columns=['news_id', 'Name', 'Sentence'])\n",
    "    aml_highrisk = np.asarray(test['content'])\n",
    "    news_ids = np.asarray(test['news_id'])\n",
    "    people_list = list(test['people_list'])\n",
    "    \n",
    "    for k, (news_id, y_news) in enumerate(zip(news_ids ,aml_highrisk)): \n",
    "        #用，。？切分句子\n",
    "        y_news = y_news.replace('。','=。')\n",
    "        y_news = y_news.replace('，','*，')\n",
    "        y_news = y_news.replace('？','+？')\n",
    "        y_news = y_news.replace('；','{；')\n",
    "        \n",
    "        news = re.split('，|。|？|；', y_news)\n",
    "        \n",
    "        news = [news.replace('=','。') for news in news]\n",
    "        news = [news.replace('*','，') for news in news]\n",
    "        news = [news.replace('+','？') for news in news]\n",
    "        news = [news.replace('{','；') for news in news]\n",
    "        news = split2textByName(news, people_list[k])\n",
    "#############################################################################################################\n",
    "        for i in range(len(people_list[k])):\n",
    "            # 找出人名存在的 news index\n",
    "            index = [index for index, _ in enumerate(news) if people_list[k][i] in _]                                \n",
    "            x = [people for people in people_list[k] if people_list[k][i] not in people]\n",
    "\n",
    "            name = [name for name in people_list[k] if name != people_list[k][i]]\n",
    "            name = [name for name in name if ((len(name)<3) & (name[0] != people_list[k][i][0])) | (len(name)>=3)]\n",
    "            name.sort(reverse=True)\n",
    "\n",
    "            #if name != []:\n",
    "            #    text = [re.sub('|'.join(name), '其他人名', news) for news in news]\n",
    "            #else:\n",
    "            #    text = news\n",
    "\n",
    "            for j in index:\n",
    "\n",
    "                mid = news[j]\n",
    "\n",
    "                if name != []:\n",
    "                    name = [name.replace('?', '\\?') for name in name]\n",
    "                    mid = re.sub('|'.join(name), '', mid)\n",
    "                \n",
    "                if j == 0:\n",
    "                    if len(news) != 1:\n",
    "                        end = news[j+1]  \n",
    "\n",
    "                        if True in [people in news[j+1] for people in x]:\n",
    "                            sentences = mid\n",
    "                        else:\n",
    "                            sentences = mid + end\n",
    "                    elif len(news) == 1:\n",
    "                        sentences = mid\n",
    "\n",
    "                elif j+1 == len(news):\n",
    "                    start = news[j-1]\n",
    "\n",
    "                    if True in [people in news[j-1] for people in x]:\n",
    "                        sentences = mid\n",
    "                    else:\n",
    "                        sentences = start + mid\n",
    "\n",
    "                else:\n",
    "                    end = news[j+1]\n",
    "                    start = news[j-1]                    \n",
    "\n",
    "                    if '。' in start:\n",
    "                        start = ''\n",
    "                    elif '。' in mid:\n",
    "                        end = ''                 \n",
    "\n",
    "                    if (True not in [people in news[j-1] for people in x]) & (True not in [people in news[j+1] for people in x]):\n",
    "                        sentences = start + mid + end\n",
    "                    elif (True not in [people in news[j-1] for people in x]) & (True in [people in news[j+1] for people in x]):\n",
    "                        sentences = start + mid\n",
    "                    elif (True in [people in news[j-1] for people in x]) & (True not in [people in news[j+1] for people in x]):\n",
    "                        sentences = mid + end\n",
    "                    else:\n",
    "                        sentences = mid    \n",
    "   \n",
    "                AML = AML.append(pd.DataFrame([[news_ids[k] ,people_list[k][i], sentences]], columns=AML.columns))\n",
    "                #break  \n",
    "              \n",
    "                \n",
    "    print('1.提取句子', datetime.now() - time)\n",
    "    time = datetime.now()\n",
    "    \n",
    "    #若 stick==True 則把多筆同姓名句子以逗點合併 (效果不好)\n",
    "    if stick:\n",
    "        AML = AML.groupby(['news_id', 'Name'])['Sentence'].apply('，'.join).reset_index()\n",
    "\n",
    "    \n",
    "    # 把指向同一人的姓名改成一樣（陳男 -> 陳水扁），若指向多人則不改（陳男 -> 陳致中、陳水扁）\n",
    "    # 將預測不完整的名字回填（王音 -> 王音之）\n",
    "    name_list = []\n",
    "    for ids in AML['news_id'].unique():\n",
    "        full_name = [name for name in AML[(AML['news_id'] == ids)]['Name']]\n",
    "        full_3name = [name for name in AML[(AML['news_id'] == ids)]['Name'] if len(name) == 3]\n",
    "        \n",
    "        a = Counter([name[0] for name in full_3name])\n",
    "        keep = [k for k,v in a.items() if v == 1]\n",
    "        full_3name_filter = [name for name in full_3name if name[0] in keep]\n",
    "        name_dict = dict((name[0], name) for name in full_3name_filter)   # ex: {'陳' : '陳水扁'}\n",
    "\n",
    "        name_dict_2 = dict(zip([name[0:2] for name in full_3name], full_3name))  # ex: {'王音': '王音之'}\n",
    "        name_dict_3 = dict(zip([name[1:] for name in full_3name], full_3name))\n",
    "        for name in full_name:\n",
    "            if (name[0] in name_dict.keys()) & (len(name) == 1):\n",
    "                name_list.append(name_dict.get(name[0]))\n",
    "            elif (name[0] in name_dict.keys()) & (len(name) == 2) & (name[-1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                                                                                  '員', '稱', '家', '哥', '媽', '生', '處',\\\n",
    "                                                                                  '和', '揆', '要', '再', '董', '涉', '母',\\\n",
    "                                                                                  '辱', '公', '少', '為', '指', '翁', '粉',\\\n",
    "                                                                                  '趁', '仔', '依', '氏', '父']):\n",
    "                name_list.append(name_dict.get(name[0]))\n",
    "            elif (name in name_dict_2.keys()) & (len(name) == 2):\n",
    "                name_list.append(name_dict_2.get(name))\n",
    "            elif (name in name_dict_3.keys()):\n",
    "                name_list.append(name_dict_3.get(name))\n",
    "            else:\n",
    "                name_list.append(name)\n",
    "                \n",
    "    print('2.整理名字', datetime.now() - time)\n",
    "    time = datetime.now()\n",
    "    \n",
    "    # 排除重複資料、排除一字、兩字簡稱、兩字三字四字姓不在姓氏表中的人\n",
    "    AML['Name'] = name_list\n",
    "    AML = AML.drop_duplicates()\n",
    "    AML = AML[AML['Name'].apply(lambda x: (len(x) > 1) )]\n",
    "    AML = AML[~AML['Name'].apply(lambda x: (len(x) == 2) & (x[1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                                                                     '員', '稱', '家', '哥', '媽', '生', '處',\\\n",
    "                                                                     '和', '揆', '要', '再', '董', '涉', '母',\\\n",
    "                                                                     '辱', '公', '少', '為', '指', '翁', '粉',\\\n",
    "                                                                     '趁', '仔', '依', '氏', '父']))]\n",
    "    \n",
    "    \n",
    "    print('3.刪除名字', datetime.now() - time)\n",
    "    time = datetime.now()\n",
    "    \n",
    "    # 第二階段 預測句子\n",
    "    input_id, segment_id, mask_input = encoded(tokenizer, AML, maxlen=maxlen)\n",
    "    prediction = model2.predict([input_id, segment_id, mask_input])\n",
    "    AML['prediction'] = prediction\n",
    "    \n",
    "    \n",
    "    # 同一人只要有一筆資料大於閥值（max），則預測為 aml 人物；若新聞中無人大於閥值，則為非 aml 新聞\n",
    "    AML['prediction'] = AML['prediction'].apply(lambda x: 0 if x < threshold else 1)\n",
    "    AML = AML.groupby(['news_id', 'Name'])['prediction'].max().reset_index()\n",
    "    AML = AML[AML['prediction'] == 1]\n",
    "    AML = AML.groupby(['news_id','prediction'])['Name'].apply(list).reset_index()\n",
    "    \n",
    "    print('4.預測名字', datetime.now() - time)\n",
    "    \n",
    "    return AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(a, b):\n",
    "    \n",
    "    if (len(a) != 0) & (len(b) != 0):\n",
    "        recall = float(len(set(a) & set(b)) / len(a))\n",
    "        pecision = float(len(set(a) & set(b)) / len(b))\n",
    "        score = 2 / (np.reciprocal(recall) + np.reciprocal(pecision))\n",
    "        return score\n",
    "    elif (len(a) == 0) & (len(b) == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 網格搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NER法]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把大於512的新聞以句點分段 (bert最多只能吃512)\n",
    "def split_content(data):\n",
    "    data_more_split = pd.DataFrame()\n",
    "    for i, row in data.iterrows():\n",
    "        if (len(row['content']) > 512) & (len(row['content']) <= 1024):\n",
    "\n",
    "            s = row['content']\n",
    "            s_split = [(i, abs(len(s)//2 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "            second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "            contents = [first, second]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content, 'aml_label':row['aml_label'], 'prediction':row['prediction']}, index=[66]), ignore_index=True)\n",
    "\n",
    "        elif len(row['content']) > 1024:\n",
    "\n",
    "            s = row['content']\n",
    "            s_split1 = [(i, abs(len(s)//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            s_split2 = [(i, abs(len(s)*2//3 - s.find(x)), x) for i, x in enumerate(s.split('。'))]\n",
    "            idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "            idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "            first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "            second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "            third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "            contents = [first, second, third]\n",
    "\n",
    "            for content in contents:\n",
    "                data_more_split = data_more_split.append(pd.DataFrame({'news_id':row['news_id'], 'content':content, 'aml_label':row['aml_label'], 'prediction':row['prediction']}, index=[66]), ignore_index=True)\n",
    "    \n",
    "    return data_more_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_ner(df, model, test, aml_threshold, stick, threshold):\n",
    "    \n",
    "    time = datetime.now()\n",
    "    \n",
    "    # 1. 預測是否疑似 aml\n",
    "    prediction = predict_aml(model, test=test, aml_threshold=aml_threshold)\n",
    "    test['prediction'] = prediction\n",
    "    aml_highrisk = test[test['prediction'] == 1]\n",
    "    \n",
    "    # 2. 將超過 512 的句子以句點拆成多句分段預測\n",
    "    test_ner = aml_highrisk.drop(['name'], axis=1)\n",
    "    data_less = test_ner[test_ner['content'].str.len() <= 512]\n",
    "    data_more = test_ner[test_ner['content'].str.len() > 512]\n",
    "    data_more_split = split_content(data_more)\n",
    "    test_ner = data_less.append(data_more_split).reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "    # 3. NER 預測人名\n",
    "    input_id, segment_id, mask_input = encoded(tokenizer, test_ner, maxlen=maxlen_ner)\n",
    "    prediction = ner_model.predict([input_id, segment_id, mask_input])\n",
    "    y_pred = np.argmax(prediction, axis=-1)\n",
    "    people_list = get_name(input_id, y_pred)\n",
    "    \n",
    "    \n",
    "    # 4. 將拆開的句子組合回去\n",
    "    test_ner['people_list'] = people_list\n",
    "    content = test_ner[['news_id', 'content', 'aml_label', 'prediction']]\n",
    "    content = content.groupby(['news_id', 'aml_label', 'prediction'])['content'].apply(lambda x : '。'.join(x)).reset_index()\n",
    "    people = test_ner[['news_id', 'aml_label', 'prediction', 'people_list']]\n",
    "    people = people.groupby(['news_id', 'aml_label', 'prediction'])['people_list'].agg(sum).reset_index()\n",
    "    people['people_list'] = [list(set(people)) for people in people['people_list']]\n",
    "    test_ner = pd.merge(content, people, on=['news_id', 'aml_label', 'prediction'], how='left')\n",
    "    \n",
    "    # 5. 將 [UNK], [PAD] 轉換回來 (王春? -> 王春甡)\n",
    "    for _, row in test_ner.iterrows():\n",
    "        for i, name in enumerate(row['people_list']):\n",
    "            if ('?' in name) | ('!' in name):\n",
    "                reexp = name.replace('?', '.').replace('!', '.')\n",
    "                reexp = re.compile(reexp,re.IGNORECASE)\n",
    "                row['people_list'][i] = re.search(reexp, row['content']).group()\n",
    "    \n",
    "    \n",
    "    print('0.CKIP', datetime.now() - time)\n",
    "    \n",
    "    # 6. 判斷名字前後句使是否為 aml\n",
    "    AML = predict_sentences(test_ner, list(test_ner['people_list']), tokenizer=tokenizer, maxlen=maxlen_sentences, stick=stick, threshold=threshold)\n",
    "    \n",
    "    test_prediction = pd.merge(test, AML[['news_id', 'Name']], on='news_id', how='left')\n",
    "    test_prediction['Name'] = test_prediction['Name'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    test_prediction['text_prediction'] = test_prediction['Name'].apply(lambda x: 0 if x == [] else 1)\n",
    "    test_prediction = test_prediction.drop(['content'],axis = 1)\n",
    "    test_prediction.columns = ['news_id', 'name', 'label', 'AML_prediction', 'Name_prediction', 'text_prediction']\n",
    "    \n",
    "    # 7. 算分數\n",
    "    score = []\n",
    "    for i in range(len(test_prediction)):\n",
    "        temp = f1_score(test_prediction['name'][i], test_prediction['Name_prediction'][i])\n",
    "        score.append(temp)\n",
    "        \n",
    "    test_prediction['f1_score'] = score    \n",
    "    total_score = sum(score)\n",
    "    aml_score = sum(test_prediction[test_prediction['label'] == 1]['f1_score'])    \n",
    "    \n",
    "    df = df.append(pd.DataFrame([[aml_threshold, threshold, stick, total_score, aml_score]], columns=df.columns))\n",
    "    \n",
    "    print('aml_threshold =', aml_threshold, 'stick =', stick, 'threshold =', threshold, 'total_score =', total_score, 'aml_score =', aml_score)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['aml_threshold', 'threshold', 'stick', 'total_score', 'aml_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.CKIP 0:00:08.457608\n",
      "1.提取句子 0:00:01.453028\n",
      "2.整理名字 0:00:00.048930\n",
      "3.刪除名字 0:00:00.003963\n",
      "4.預測名字 0:00:08.713954\n",
      "aml_threshold = 0.4 stick = False threshold = 0.4 total_score = 80.34603174603177 aml_score = 61.346031746031734\n"
     ]
    }
   ],
   "source": [
    "for aml_threshold in [0.4]:\n",
    "    for threshold in [0.4]:\n",
    "        for stick in [False]:        \n",
    "            df = GridSearch_ner(df, model, test, aml_threshold=aml_threshold, stick=stick, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '收受、樂揚建設總經理鄒雪娥贈送的2支香奈兒名表、蕭邦名表；還在的生日過後幾個月，'\n",
    "input_id, segment_id = tokenizer.encode(content, max_len=maxlen)\n",
    "mask_input = [transfer(i) for i in input_id]\n",
    "prediction = model.predict([[input_id], [segment_id], [mask_input]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = '收受、樂揚建設總經理鄒雪娥贈送的2支香奈兒名表、蕭邦名表；還在的生日過後幾個月，'\n",
    "input_id, segment_id = tokenizer.encode(content, max_len=maxlen)\n",
    "mask_input = [transfer(i) for i in input_id]\n",
    "prediction = model2.predict([[input_id], [segment_id], [mask_input]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'C:\\Users\\rocker\\extra data\\人肉爬蟲_20200615.csv', encoding='cp950')\n",
    "data2['name'] = data2['name'].apply(lambda x: eval(x))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "test = data2\n",
    "test = test[['news_id', 'name', 'content', 'aml_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'E:\\Desktop\\檢察官_法官_filter3 - 複製.csv', encoding='cp950')\n",
    "data2['aml_label'] = data2['people_list'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data2 = data2.drop(['title', 'link'], axis=1)\n",
    "data2['people_list'] = data2['people_list'].apply(lambda x: eval(x))\n",
    "data2['news_id'] = range(10000, 10000+len(data2))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', x))\n",
    "data2['content'] = data2['content'].apply(lambda x: x.strip().replace('記者', '＜')\n",
    "                                                   .replace('報導', '＞')\n",
    "                                                   .replace('▲', '')\\\n",
    "                                                   .replace('。　', '。')\\\n",
    "                                                   .replace('\b', '')\\\n",
    "                                                   .replace('.', '')\\\n",
    "                                                   .replace(' ', '')\\\n",
    "                                                   .replace('“', '「')\\\n",
    "                                                   .replace('”', '」'))\n",
    "data2['content'] = data2['content'].apply(lambda x: re.sub('＜[^＞]*＞', '', x))\n",
    "data2.columns = ['news_id', 'content', 'aml_label', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv(r'E:\\Desktop\\檢察官_法官_filter.csv', encoding='cp950')\n",
    "data3 = data3.drop(['title', 'link'], axis=1)\n",
    "data3['aml_label'] = data3['people_list'].apply(lambda x: 0 if x == '[]' else 1)\n",
    "data3['people_list'] = data3['people_list'].apply(lambda x: eval(x))\n",
    "data3['news_id'] = range(10000, 10000+len(data3))\n",
    "data3.columns = ['news_id', 'content', 'aml_label', 'name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.append(data2).append(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.news_id = range(0, len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
